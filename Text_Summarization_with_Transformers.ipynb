{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this project, you’ll use the BART model by Facebook to generate summaries of news stories in the BBC news dataset. You’ll perform the following steps:\n",
        "\n",
        "Use the PyTorch implementation available at Hugging Face Hub.\n",
        "Load the dataset, and write the dataset class.\n",
        "Use BART’s tokenizer to create the DataLoaders.\n",
        "Load the model from Hugging Face’s Transformers library, and write its training script.\n",
        "Train the model and use the Weights & Biases library to monitor the training process.\n",
        "Write the validation script and test the model on the test set.\n",
        "Use the Evaluate library to compute the ROUGE evaluation metric.\n",
        "A Jupyter Notebook has been provided in the code editor available on the right side of the workspace. Open the BART-transformer_summarization.ipynb file using the file tree panel of the editor.\n",
        "\n",
        "Every task in the project has an associated cell in the notebook that the heading can identify over it."
      ],
      "metadata": {
        "id": "T2t6SzVluZtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jI8aUUzsjQ2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import cuda\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import __version__, BartTokenizer, BartForConditionalGeneration\n",
        "from huggingface_hub import interpreter_login\n",
        "import evaluate\n",
        "import wandb\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "r5xfMLD7vKUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter_login()"
      ],
      "metadata": {
        "id": "ysqHUtxlwGdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "VPVzLOoCwIw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"BART_summarization\")\n",
        "\n",
        "config = wandb.config\n",
        "config.TRAIN_BATCH_SIZE = 2\n",
        "config.VALID_BATCH_SIZE = 2\n",
        "config.TRAIN_EPOCHS = 2\n",
        "config.LEARNING_RATE = 1e-4\n",
        "config.SEED = 42\n",
        "config.MAX_LEN = 512\n",
        "config.SUMMARY_LEN = 150\n",
        "\n",
        "torch.manual_seed(config.SEED)\n",
        "np.random.seed(config.SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "new_repo = \"Text-Summarization-with-Transformers\"\n",
        "repo_name = \"EducativeCS2023/bart-base-summarization-trained\""
      ],
      "metadata": {
        "id": "HFwlf0ZvwK5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/usercode/BBCarticles.csv' ,encoding='latin-1')\n",
        "df = df[['Text','Summary']]\n",
        "df.Text = 'summarize: ' + df.Text\n",
        "\n",
        "split = 0.025\n",
        "train_dataset=df.sample(frac=split,random_state = config.SEED)\n",
        "eval_dataset=df.drop(train_dataset.index).sample(frac=split,random_state = config.SEED).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"train_dataset\", train_dataset.shape)\n",
        "print(\"eval_dataset\", eval_dataset.shape)\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "awTmp0LPxDt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.Summary = self.data.Summary\n",
        "        self.Text = self.data.Text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Summary)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        Text = str(self.Text[index])\n",
        "        Text = ' '.join(Text.split())\n",
        "\n",
        "        Summary = str(self.Summary[index])\n",
        "        Summary = ' '.join(Summary.split())\n",
        "\n",
        "        source = self.tokenizer([Text], max_length= self.source_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        target = self.tokenizer([Summary], max_length= self.summ_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "CYphFywLxMhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(repo_name)\n",
        "\n",
        "tokenizer.push_to_hub(new_repo)\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "eval_set = CustomDataset(eval_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "training_loader = DataLoader(training_set, batch_size=config.TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "eval_loader = DataLoader(eval_set, batch_size=config.VALID_BATCH_SIZE, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "2Ro5QqZSxmOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(repo_name)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OOtWVstkxsND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        labels = y[:, 1:].clone().detach()\n",
        "        labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        if _%10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-pp7OBM8yups"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(config.TRAIN_EPOCHS):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "model.push_to_hub(new_repo)"
      ],
      "metadata": {
        "id": "uMjMEAuWy0Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask,\n",
        "                max_length=150,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "metadata": {
        "id": "LXthrTX1y2Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tik = time.time()\n",
        "predictions, actuals = predict(tokenizer, model, device, eval_loader)\n",
        "results = pd.DataFrame({'predictions':predictions,'actuals':actuals})\n",
        "results.to_csv('predictions.csv')\n",
        "tok = time.time()\n",
        "print(\"time taken\", tok-tik, \"seconds\")\n",
        "results.head()"
      ],
      "metadata": {
        "id": "666gO5bSy4q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_score = evaluate.load(\"rouge\")\n",
        "\n",
        "scores = rouge_score.compute(\n",
        "    predictions=results['predictions'], references=results['actuals']\n",
        ")\n",
        "pd.DataFrame([scores]).T.head()"
      ],
      "metadata": {
        "id": "1yDJXnHwy6pr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}